1.tanh激活函数通常比隐藏层单元的sigmoid激活函数效果更好，因为其输出的平均值更接近于零，因此它将数据集中在下一层是更好的选择，请问正确吗？

- 【`★`】True
- 【 】 False

> tanh的输出在-1和1之间，因此它将数据集中在一起，使得下一层的学习变得更加简单。



2.您正在构建一个识别黄瓜（y = 1）与西瓜（y = 0）的二元分类器。 你会推荐哪一种激活函数用于输出层？

- 【 】 ReLU
- 【 】 Leaky ReLU
- 【`★`】sigmoid
- 【 】 tanh

注意：来自sigmoid函数的输出值可以很容易地理解为概率。

> Sigmoid输出的值介于0和1之间，这使其成为二元分类的一个非常好的选择。 如果输出小于0.5，则可以将其归类为0，如果输出大于0.5，则归类为1。 它也可以用tanh来完成，但是它不太方便，因为输出在-1和1之间。



3.假设你已经建立了一个神经网络。 您决定将权重和偏差初始化为零。 以下哪项陈述是正确的？

【★】第一个隐藏层中的每个神经元节点将执行相同的计算。 所以即使经过多次梯度下降迭代后，层中的每个神经元节点都会计算出与其他神经元节点相同的东西。
【 】 第一个隐藏层中的每个神经元将在第一次迭代中执行相同的计算。 但经过一次梯度下降迭代后，他们将学会计算不同的东西，因为我们已经“破坏了对称性”。
【 】第一个隐藏层中的每一个神经元都会计算出相同的东西，但是不同层的神经元会计算不同的东西，因此我们已经完成了“对称破坏”。
【 】即使在第一次迭代中，第一个隐藏层的神经元也会执行不同的计算， 他们的参数将以自己的方式不断发展。

> 每个神经元结点相同不代表没有学习到有用的决策边界



4.Logistic回归的权重w应该随机初始化，而不是全零，因为如果初始化为全零，那么逻辑回归将无法学习到有用的决策边界，因为它将无法“破坏对称性”，是正确的吗？

- 【 】True
- 【`★`】False

> 逻辑回归没有隐藏层。如果将权重初始化为零，输入第一个样本x，模型将输出零。
> 但逻辑回归的导数取决于输入x（因为没有隐藏层），并且输入x不是零。
> 所以在第二次迭代中，权重W值遵循x的分布，并且如果x不是一个常数向量，那么它们(w1,w2,...)之间是不同的。



5.看一下下面的单隐层神经网络：

![1](./imgs/1.jpg)

- [ ] W[1] 的维度是 (2, 4)
- [x] b[1] 的维度是 (4, 1)
- [x] W[1] 的维度是 (4, 2)
- [ ] b[1] 的维度是 (2, 1)
- [x] W[2] 的维度是 (1, 4)
- [ ] b[2] 的维度是 (4, 1)
- [ ] W[2] 的维度是 (4, 1)
- [x] b[2] 的维度是 (1, 1)



6.在和上一个相同的网络中，Z[1] 和 A[1]的维度是多少？

- [ ] Z[1] 和 A[1] 的维度都是 (4, 1)

- [ ] Z[1] 和 A[1] 的维度都是 (1, 4)

- [x] Z[1] 和 A[1] 的维度都是 (4, m)
    ```diff
    m为样本数
    ```
    
- [ ] Z[1] 和 A[1] 的维度都是 (4, 2)



7.下列哪个说法是正确的？

- [x] 神经网络的更深层通常比前面层计算更复杂的输入特征。
- [ ] 神经网络的前面层通常比更深层计算更复杂的输入特性。



8.下面关于神经网络的说法正确的是：
![neuralNetworkGraph](/home/bian/document/ai/1.4一步步搭建多层神经网络以及应用(1&2)/imgs/neuralNetworkGraph.jpg)

- [x] 总层数L为4，隐藏层层数为3。
    ```diff
    网络层数按隐藏层数+1计算。输入和输出层不算作隐藏层。
    ```
    
- [ ] 总层数L为3，隐藏层层数为3。

- [ ] 总层数L为4，隐藏层层数为4。

- [ ] 总层数L为5，隐藏层层数为4。

```diff
一般来说
W[l]的形状是（n[l]，n[l-1]）
b[l]的形状是（n[l]，1）
```



9.在前向传播期间，在层l的前向传播函数中，您需要知道层l中的激活函数（Sigmoid，tanh，ReLU等）是什么， 在反向传播期间，相应的反向传播函数也需要知道第l层的激活函数是什么，因为梯度是根据它来计算的。这样描述正确吗？

- [x] 正确
- [ ] 错误
    ```diff
    不同激活函数有不同的导数。在反向传播期间，需要知道正向传播中使用哪种激活函数才能计算正确的导数。
    ```



10.有一些功能具有以下属性：

>(i) 利用浅网络电路计算一个函数时，需要一个大网络（我们通过网络中的逻辑门数量来度量大小），但是（ii）使用深网络电路来计算它，只需要一个指数较小的网络。真/假？
- [x] 正确
    ```diff
    深层的网络隐藏单元数量相对较少，隐藏层数目较多，
    如果浅层的网络想要达到同样的计算结果则需要指数级增长的单元数量才能达到。
    ```
- [ ] 错误